#!/usr/bin/env python3

import asyncssh
import argparse
import requests
import asyncio
import logging
import random
import json
import sys

logger = logging.getLogger(__file__)


def _get_file_io(name, write=False):
    if name == '-' and write:
        return sys.stdout
    elif name == '-' and not write:
        return sys.stdin
    else:
        return open(name, 'w' if write else 'r')

class ScpHandler():
    def __init__(self, source=None, target=None):
        self.source = source
        self.target = target

    @classmethod
    def parse_args(cls, parser):
        parser.add_argument(
            'source',
            nargs='?',
            help='File copy source'
        )
        parser.add_argument(
            'target',
            nargs='?',
            help='File copy target'
        )

    async def __call__(self, entry):
        source = self.source.format(entry=entry)
        target = self.target.format(entry=entry)
        async with asyncssh.connect(entry, known_hosts=None) as conn:
            if self.source.startswith('{entry}:'):
                source = (conn, source.split(':', 1)[1])
            if self.target.startswith('{entry}:'):
                target = (conn, target.split(':', 1)[1])
            await asyncssh.scp(source, target)
        return {
            'success': True,
            'source': self.source.format(entry=entry),
            'target': self.target.format(entry=entry),
        }

class HTTPHandler():
    def __init__(self, url=None, method=None):
        self.url = url
        self.method = method

    @classmethod
    def parse_args(cls, parser):
        parser.add_argument(
            'url',
            nargs='?',
            default='http://{entry}',
            help='URL of resource (defaults to http://{entry})'
        )
        parser.add_argument(
            '-m', '--method',
            default='GET',
            help='HTTP Method to use (GET/POST, etc)'
        )

    def __call__(self, entry):
        url = self.url.format(entry=entry)
        response = requests.request(self.method, url)
        print(response.text)
        return response.text

class SSHHandler():
    NO_SUDO = object()

    def __init__(self, command=None, continue_on_error=True, as_user=None):
        self.command = command
        self.continue_on_error = continue_on_error

        # Hackery around argparse's "empty" value being None for present arg
        # while the actual missing arg takes on the default defined in the 
        # `add_argument` function
        if as_user == self.NO_SUDO:
            self.as_user = None
        elif not as_user:
            self.as_user = 'root'
        else:
            self.as_user = as_user

    @classmethod
    def parse_args(cls, parser):
        parser.add_argument(
            'command',
            nargs='?',
            help='Command to run'
        )
        parser.add_argument(
            '--continue-on-error',
            action='store_true',
            default=False,
            help='Continue executing commands if non-success exit status is given',
        )
        parser.add_argument(
            '--sudo',
            nargs='?',
            default=cls.NO_SUDO,
            dest='as_user',
            help='Sudo as user'
        )

    async def __call__(self, entry):
        command_results = []
        commands = [self.command]
        host = entry.strip()
        async with asyncssh.connect(host, known_hosts=None) as conn:
            for command in commands:
                if self.as_user:
                    command = 'sudo -u {} {}'.format(self.as_user, command)
                result = await conn.run(command)
                command_results.append({
                    'host': entry,
                    'command': command,
                    'exit_code': result.exit_status,
                    'stdout': result.stdout,
                    'stderr': result.stderr,
                })
                if result.exit_status != 0 and not self.continue_on_error:
                    break

        success = all([r['exit_code'] == 0 for r in command_results])
        if len(commands) == 1:
            return { 'success': success, **command_results[0] }
        return {
            'success': success,
            'commands': command_results,
        }

async def test_handler(entry):
    wait_value = random.randint(1,5)
    await asyncio.sleep(wait_value)
    return wait_value
  
class Nexus():
    """
        :: Worker-oriented event loop processor

        Workers are no longer a necessary concept in an asynchronous world. However, the concept can still be very
        helpful for controlling resource usage on the host machine or remote systems used by the job. For this reason
        I'm re-implementing a worker-style executor pool which could be used with jobs that are threaded or async.
    """
    DEFAULT_WORKERS = 5

    def __init__(self, source, target, handler, stream_output=False, show_headers=False, show_progress=True, workers=DEFAULT_WORKERS):
        self.source = source
        self.target = target
        self.loop = asyncio.get_event_loop()
        self.handler = handler
        self.queue = asyncio.Queue(loop=self.loop)
        self.show_progress = show_progress and not stream_output
        self.show_headers = show_headers
        self.stream_output = stream_output
        self.worker_count = workers

        # State data
        self.entry_count = 0
        self.active_count = 0
        self.all_enqueued = False

        # Results
        self.results = []

    def _show_progress(self, newline=False):
         if not self.show_progress:
             return

         count_complete = len(self.results)
         sys.stdout.write('\r {done} out of {total} tasks complete - {percent_done}% (running={running}; pending={pending})'.format(
             done=count_complete,
             total=self.entry_count,
             percent_done=int(count_complete / self.entry_count * 100),
             running=self.active_count,
             pending=self.queue.qsize(),
         ))
         if newline:
             # Finish with a newline
             print('')

    def run(self):
        self.loop.run_until_complete(self._execute_jobs())

    def _is_complete(self):
        return len(self.results) == self.entry_count

    async def _execute_jobs(self):
        # Enqueue all work tasks
        for entry in self.source:
            self.entry_count += 1
            self.queue.put_nowait(entry.rstrip('\n'))
        self.all_enqueued = True

        # Start workers
        workers = []
        for i in range(self.worker_count):
            workers.append(self.loop.create_task(self._worker()))

        # Wait and show progress
        while True:
            self._show_progress()
            await asyncio.wait(workers, timeout=0.1, loop=self.loop)
            if self._is_complete():
                break
            else:
                self._show_progress()
        self._show_progress(newline=True)
        if not self.stream_output:
            for output_entry in self.results:
                self._output_entry(output_entry)
        return self.results

    def _output_entry(self, result_entry):
        entry, result = result_entry
        if isinstance(result, (dict, list)):
            output_entry = json.dumps(result)
        else:
            output_entry = str(result)
        output_entry = output_entry + '\n'
        if self.show_headers:
            output_entry = f'{entry}: {output_entry}'
        self.target.write(output_entry)

    async def _worker(self):
        while True:
            try:
                entry = await asyncio.wait_for(self.queue.get(), timeout=.1, loop=self.loop)
            except asyncio.TimeoutError:
                if self.queue.qsize() == 0 and self.all_enqueued:
                    return
                else:
                    continue
            self.active_count += 1
            try:
                if asyncio.iscoroutinefunction(self.handler):
                    result = await self.handler(entry)
                else:
                    print('starting to run in executor')
                    def handler_wrapper():
                        return self.handler(entry)
                    result = await self.loop.run_in_executor(None, handler_wrapper)
                    print('done running in executor')
            except Exception as e:
                result = e
            result_entry = (entry, result)
            self.results.append(result_entry)
            if self.stream_output:
                self._output_entry(result_entry)
            self.queue.task_done()
            self.active_count -= 1

HANDLERS = {
    'http': HTTPHandler,
    'test': test_handler,
    'ssh': SSHHandler,
    'scp': ScpHandler,
}

def crank_command():
    cmd_parser = argparse.ArgumentParser(prog='crank')
    cmd_parser.add_argument('--input', help='Set source (Default stdin)', default='-')
    cmd_parser.add_argument('--output', help='Set target of output (Default stdout)', default='-')
    cmd_parser.add_argument(
        '-n', '--headers-only',
        help='Only show input values with truthy non-exception results (useful with --filter)',
        action='store_true',
        default=False,
    )
    cmd_parser.add_argument(
        '-m', '--module',
        help='Operational Module',
        choices=HANDLERS.keys(),
        default='ssh',
    )
    cmd_parser.add_argument(
        '-w', '--workers',
        type=int,
        help='Number of concurrent workers',
        default=10,
    )
    cmd_parser.add_argument(
        '-d', '--headers',
        action='store_true',
        help='Prefix output with entry name',
        default=False,
    )
    cmd_parser.add_argument(
        '-p', '--progress',
        action='store_true',
        help='Output progress bar and dont display results until the end',
        default=False,
    )
    args, extra_args = cmd_parser.parse_known_args()
    source = _get_file_io(args.input)
    target = _get_file_io(args.output, write=True)

    Handler = HANDLERS.get(args.module)

    handler_parser = argparse.ArgumentParser()
    if hasattr(Handler, 'parse_args'):
        Handler.parse_args(handler_parser)
        handler_args = handler_parser.parse_args(extra_args)
        handler = Handler(**handler_args.__dict__)
    else:
        handler = Handler

    try:
        nexus = Nexus(
            source,
            target,
            handler,
            show_headers=args.headers,
            stream_output=not args.progress,
            workers=args.workers
        )
        nexus.run()
    except Exception as e:
        logging.exception('Stream io failed:')
    finally:
        if hasattr(source, 'close'):
            source.close()
        if hasattr(target, 'close'):
            target.close()

if __name__ == '__main__':
    crank_command()
